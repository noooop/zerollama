# Yi
Yi ç³»åˆ—æ¨¡å‹æ˜¯ [01.AI](https://01.ai/) ä»é›¶è®­ç»ƒçš„ä¸‹ä¸€ä»£å¼€æºå¤§è¯­è¨€æ¨¡å‹ã€‚

##  TL;DR
- Yi ç³»åˆ—æ¨¡å‹æ˜¯ä¸€ä¸ªåŒè¯­è¯­è¨€æ¨¡å‹ï¼Œåœ¨ 3T å¤šè¯­è¨€è¯­æ–™åº“ä¸Šè®­ç»ƒè€Œæˆï¼Œæ˜¯å…¨çƒæœ€å¼ºå¤§çš„å¤§è¯­è¨€æ¨¡å‹ä¹‹ä¸€ã€‚Yi ç³»åˆ—æ¨¡å‹åœ¨è¯­è¨€è®¤çŸ¥ã€å¸¸è¯†æ¨ç†ã€é˜…è¯»ç†è§£ç­‰æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚ä¾‹å¦‚ï¼Œ

- Yi-34B-Chat æ¨¡å‹åœ¨ AlpacaEval Leaderboard [æ’åç¬¬äºŒ](https://twitter.com/01AI_Yi/status/1745371506623103087?s=20)ï¼Œ**ä»…æ¬¡äº GPT-4 Turbo**ï¼Œè¶…è¿‡äº† GPT-4ã€Mixtral å’Œ Claude ç­‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆæ•°æ®æˆªæ­¢è‡³ 2024 å¹´ 1 æœˆï¼‰ã€‚

- Yi-34B æ¨¡å‹åœ¨ [Hugging Face Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)ï¼ˆé¢„è®­ç»ƒï¼‰ä¸ C-Eval åŸºå‡†æµ‹è¯•ä¸­[è£ç™»æ¦œé¦–](https://mp.weixin.qq.com/s/tLP-fjwYHcXVLqDcrXva2g)ï¼Œ**åœ¨ä¸­æ–‡å’Œè‹±æ–‡è¯­è¨€èƒ½åŠ›æ–¹é¢**å‡è¶…è¿‡äº†å…¶å®ƒå¼€æºæ¨¡å‹ï¼Œä¾‹å¦‚ï¼ŒFalcon-180Bã€Llama-70B å’Œ Claudeï¼ˆæ•°æ®æˆªæ­¢è‡³ 2023 å¹´ 11 æœˆï¼‰ã€‚

- ï¼ˆè‡´è°¢ Llama ï¼‰æ„Ÿè°¢ Transformer å’Œ Llama å¼€æºç¤¾åŒºï¼Œä¸ä»…ç®€åŒ–äº†å¼€å‘è€…ä»é›¶å¼€å§‹æ„å»ºå¤§æ¨¡å‹çš„å·¥ä½œï¼Œå¼€å‘è€…è¿˜å¯ä»¥åˆ©ç”¨ Llama ç”Ÿæ€ä¸­ç°æœ‰çš„å·¥å…·ã€åº“å’Œèµ„æºï¼Œæé«˜å¼€å‘æ•ˆç‡ã€‚


# Yi-1.5

##  TL;DR

Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples. 

Compared with Yi, Yi-1.5 delivers stronger performance in coding, math, reasoning, and instruction-following capability, while still maintaining excellent capabilities in language understanding, commonsense reasoning, and reading comprehension. 

Yi-1.5 comes in 3 model sizes: 34B, 9B, and 6B. For model details and benchmarks, see [Model Card](https://huggingface.co/collections/01-ai/yi-15-2024-05-663f3ecab5f815a3eaca7ca8).



## Todo
GPTQ-Int8 æ¨¡å‹æŠ¥é”™ åŒ qwen 1.5

## News
- 2024-05-13: The Yi-1.5 series models are open-sourced, further improving coding, math, reasoning, and instruction-following abilities. 

ğŸ¯ 2024-03-16ï¼šå‘å¸ƒå¹¶å¼€æºäº† Yi-9B-200K æ¨¡å‹ã€‚

ğŸ¯ 2024-03-08: å‘å¸ƒäº† Yi æŠ€æœ¯æŠ¥å‘Šï¼

ğŸ”” 2024-03-07: å¢å¼ºäº† Yi-34B-200K é•¿æ–‡æœ¬è®°å¿†å’Œæ£€ç´¢èƒ½åŠ›ã€‚

Yi-34B-200K çš„â€œå¤§æµ·æé’ˆâ€èƒ½åŠ›å¢å¼ºäº† 10.5%, ä» 89.3% æå‡åˆ°äº† 99.8%ã€‚ åœ¨ 5B tokens çš„é•¿æ–‡æœ¬æ•°æ®é›†ä¸Šï¼Œå¯¹æ¨¡å‹è¿›è¡Œç»§ç»­é¢„è®­ç»ƒï¼Œæ¨¡å‹æ€§èƒ½è¾¾åˆ°é¢„æœŸç›®æ ‡ã€‚

ğŸ¯ 2024-03-06: å‘å¸ƒå¹¶å¼€æºäº† Yi-9B æ¨¡å‹ã€‚

Yi-9B æ¨¡å‹åœ¨ Mistral-7Bã€SOLAR-10.7Bã€Gemma-7Bã€DeepSeek-Coder-7B-Base-v1.5 ç­‰ç›¸è¿‘å°ºå¯¸çš„æ¨¡å‹ä¸­ååˆ—å‰èŒ…ï¼Œå…·æœ‰å‡ºè‰²çš„ä»£ç èƒ½åŠ›ã€æ•°å­¦èƒ½åŠ›ã€å¸¸è¯†æ¨ç†èƒ½åŠ›ä»¥åŠé˜…è¯»ç†è§£èƒ½åŠ›ã€‚

ğŸ¯ 2024-01-23: å‘å¸ƒå¹¶å¼€æºäº† Yi-VL-34B å’Œ Yi-VL-6B å¤šæ¨¡æ€è¯­è¨€å¤§æ¨¡å‹ã€‚

Yi-VL-34Båœ¨ MMMU å’Œ CMMMU æœ€æ–°çš„åŸºå‡†æµ‹è¯•ä¸­è£ç™»æ¦œé¦–ï¼ˆæ•°æ®æˆªæ­¢è‡³ 2024 å¹´ 1 æœˆï¼‰ã€‚

ğŸ¯ 2023-11-23: å‘å¸ƒå¹¶å¼€æºäº†å…­å¤§ Chat æ¨¡å‹ã€‚

ğŸ”” 2023-11-23ï¼š Yi ç³»åˆ—æ¨¡å‹ç¤¾åŒºè®¸å¯åè®®æ›´æ–°è‡³ 2.1 ç‰ˆæœ¬ã€‚ 

ğŸ”¥ 2023-11-08ï¼š Yi-34B-Chat æ¨¡å‹å¼€å§‹é‚€è¯·æµ‹è¯•ã€‚

ğŸ¯ 2023-11-05ï¼š å‘å¸ƒå¹¶å¼€æºäº† Yi-6B-200K å’Œ Yi-34B-200K Base æ¨¡å‹ã€‚

ğŸ¯ 2023-11-02ï¼š å‘å¸ƒå¹¶å¼€æºäº† Yi-6B-Base å’Œ Yi-34B-Base æ¨¡å‹ã€‚


## License Agreement

The source code in this repo is licensed under the Apache 2.0 license. The Yi series models are fully open for academic research and free for commercial use, with automatic permission granted upon application. All usage must adhere to the Yi Series Models Community License Agreement 2.1. For free commercial use, you only need to send an email to get official commercial permission.

æœ¬ä»“åº“ä¸­çš„æºä»£ç éµå¾ª Apache 2.0 è®¸å¯è¯ã€‚Yi ç³»åˆ—æ¨¡å‹å®Œå…¨å¼€æ”¾ï¼Œä½ å¯ä»¥å…è´¹ç”¨äºä¸ªäººç”¨é€”ã€å­¦æœ¯ç ”ç©¶å’Œå•†ä¸šç”¨é€”ã€‚å¦‚éœ€å•†ç”¨ï¼Œä½ ä»…éœ€æäº¤ç”³è¯·ï¼Œå³èƒ½ç«‹åˆ»è‡ªåŠ¨è·å– Yi ç³»åˆ—æ¨¡å‹å•†ç”¨è®¸å¯ï¼Œè€Œæ— éœ€ç­‰å¾…å®˜æ–¹å®¡æ‰¹ã€‚æ‰€æœ‰ä½¿ç”¨å¿…é¡»éµå®ˆã€ŠYiç³»åˆ—æ¨¡å‹ç¤¾åŒºè®¸å¯åè®® 2.1ã€‹ã€‚


## Reference
[GITHUB](https://github.com/01-ai/Yi/)

[Hugging Face](https://huggingface.co/01-ai/)

[ModelScope](https://www.modelscope.cn/organization/01ai/)

[Yi Tech Report](https://arxiv.org/abs/2403.04652)

