{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af1c0ddd-4e0f-415c-9585-a1d81ad98080",
   "metadata": {},
   "source": [
    "# 7. use tools 使用工具"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f1ad12-517b-4e71-b9f0-3083c0fd8e7c",
   "metadata": {},
   "source": [
    "首先将zerollama目录加入python path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72a9a1c5-487a-4e6e-9885-ea0ddff0eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "pwd = Path(os.getcwd())\n",
    "sys.path.append(str(pwd.parent.parent.parent))\n",
    "os.chdir(str(pwd.parent.parent.parent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df05786-31dc-484f-9bab-f010e705e685",
   "metadata": {},
   "source": [
    "导入模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24282323-86cc-43a3-9116-d171a43fdf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from zerollama.agents.use_tool.agent_use_tools import AgentUseTools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7becab6b-e437-4b63-bebc-34089d3a9b59",
   "metadata": {},
   "source": [
    "定义第一个工具 查询当前天气，当然为了测试，温度是写死的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecc7f3db-4ee6-49fd-a99f-70bf3df002a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_temperature(location: str, unit: str=\"celsius\") -> str:\n",
    "    \"\"\"\n",
    "    Get the current temperature at a location.\n",
    "\n",
    "    Args:\n",
    "        location: The location to get the temperature for, in the format \"City, Country\"\n",
    "        unit: The unit to return the temperature in. (choices: [\"celsius\", \"fahrenheit\"])\n",
    "    Returns:\n",
    "        The current temperature at the specified location in the specified units.\n",
    "    \"\"\"\n",
    "    if \"chicago\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Chicago\", \"temperature\": \"13\", \"unit\": unit})\n",
    "    elif \"san francisco\" in location.lower():\n",
    "        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"55\", \"unit\": unit})\n",
    "    elif \"new york\" in location.lower():\n",
    "        return json.dumps({\"location\": \"New York\", \"temperature\": \"11\", \"unit\": unit})\n",
    "    else:\n",
    "        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e303cb7a-2a7a-487a-af47-7df42ab4cf22",
   "metadata": {},
   "source": [
    "第二个工具 查询当前风速，当然风速也是写死的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89ed650c-9e51-4e9d-9c4e-80a90dd07794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_wind_speed(location: str) -> float:\n",
    "    \"\"\"\n",
    "    Get the current wind speed in km/h at a given location.\n",
    "\n",
    "    Args:\n",
    "        location: The location to get the temperature for, in the format \"City, Country\"\n",
    "    Returns:\n",
    "        The current wind speed at the given location in km/h, as a float.\n",
    "    \"\"\"\n",
    "    return 6.  # A real function should probably actually get the wind speed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a2c14a-d748-4825-9ea9-a5c40c9a6bf2",
   "metadata": {},
   "source": [
    "我们是用NousResearch/Hermes-2-Pro-Llama-3-8B，据说是同尺寸模型里，工具调用最好的之一。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6e83390-14c3-439c-94f8-0d0f63da64b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"NousResearch/Hermes-2-Pro-Llama-3-8B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb13c3c-3b3d-442c-955c-f0d7c4d5a1fe",
   "metadata": {},
   "source": [
    "可以使用openai和zerollama接口形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dde93312-d7bc-4d50-b21c-dcf761455c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zerollama\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Chat [NousResearch/Hermes-2-Pro-Llama-3-8B] server not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(llm_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     12\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     13\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the current temperature of New York, San Francisco and Chicago?\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     14\u001b[0m ]\n\u001b[0;32m---> 16\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(reply)\n",
      "File \u001b[0;32m/media/noooop/cache/share/PycharmProjects/zerollama/zerollama/agents/use_tool/agent_use_tools.py:26\u001b[0m, in \u001b[0;36mAgentUseTools.generate_reply\u001b[0;34m(self, messages, stream, options)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_reply\u001b[39m(\u001b[38;5;28mself\u001b[39m, messages, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(messages, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m---> 26\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_system_message\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools_schema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(messages, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     28\u001b[0m         messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m}]\n",
      "File \u001b[0;32m/media/noooop/cache/share/PycharmProjects/zerollama/zerollama/agents/core/chat_client.py:18\u001b[0m, in \u001b[0;36mZerollamaChatClient.chat\u001b[0;34m(self, messages, tools, stream, options)\u001b[0m\n\u001b[1;32m     16\u001b[0m options \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[1;32m     17\u001b[0m options\u001b[38;5;241m.\u001b[39mupdate(options \u001b[38;5;129;01mor\u001b[39;00m {})\n\u001b[0;32m---> 18\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/media/noooop/cache/share/PycharmProjects/zerollama/zerollama/tasks/chat/engine/client.py:29\u001b[0m, in \u001b[0;36mChatClient.chat\u001b[0;34m(self, name, messages, tools, stream, options)\u001b[0m\n\u001b[1;32m     27\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery(name, method, data)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChat [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] server not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misgenerator(response):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Chat [NousResearch/Hermes-2-Pro-Llama-3-8B] server not found."
     ]
    }
   ],
   "source": [
    "for llm_config in [\n",
    "    {\"type\": \"zerollama\", \"model\": model, \"global_priority\": False},\n",
    "    {\"type\": \"openai\", \"model\": model, \"base_url\": 'http://localhost:8080/v1/', \"global_priority\": False},\n",
    "]:\n",
    "    agent = AgentUseTools(\n",
    "        system_message=\"You are a bot that responds to weather queries. You should reply with the unit used in the queried location.\",\n",
    "        tools=[get_current_temperature, get_current_wind_speed],\n",
    "        llm_config=llm_config\n",
    "    )\n",
    "    print(llm_config[\"type\"])\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"What is the current temperature of New York, San Francisco and Chicago?\"}\n",
    "    ]\n",
    "\n",
    "    reply = agent.generate_reply(messages)\n",
    "\n",
    "    print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16808be-c43d-4a13-b361-cd1e381d1116",
   "metadata": {},
   "source": [
    "# 总结\n",
    "能使用工具为llm如虎添翼。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35deb0df-e259-4c06-a9c9-833e9a3ab3a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
